#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.10"
# dependencies = []
# ///
"""
ralph-loop - Run a Ralph Wiggum loop on a spec file.

Iteratively runs claude -p to work through spec tasks until complete.
Supports two spec formats:
  1. Markdown specs with "## Step by Step Tasks" section (h3 headers as tasks)
  2. JSON PRD files with a "stories" array (legacy format)

Exits when: all tasks complete, max-iterations reached, or completion promise detected.

Usage:
    ralph-loop --spec <file> [--max-iterations N] [--completion-promise TEXT] [--model MODEL] [--log FILE]
    ralph-loop --prd <file> [--max-iterations N] [--completion-promise TEXT] [--model MODEL] [--log FILE]  (legacy JSON format)

Examples:
    ralph-loop --spec specs/feature.md
    ralph-loop --spec specs/feature.json --max-iterations 10
    ralph-loop --prd legacy.json --completion-promise "ALL DONE"
    ralph-loop --spec specs/feature.md --model claude --log /tmp/ralph.log
"""

import argparse
import json
import os
import re
import subprocess
import sys
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

# Try to import StructuredLogger
try:
    from lib.logger import StructuredLogger
except ImportError:
    # Try adding lib directory to path
    script_dir = Path(__file__).parent
    lib_dir = script_dir.parent / "lib"
    if lib_dir.exists():
        sys.path.insert(0, str(lib_dir.parent))
        try:
            from lib.logger import StructuredLogger
        except ImportError:
            StructuredLogger = None
    else:
        StructuredLogger = None


def log(step: str, message: str) -> None:
    """Log a message with step indicator."""
    print(f"[{step}] {message}", flush=True)


# ============================================================================
# Output Formatting Utilities
# ============================================================================

def supports_color() -> bool:
    """
    Check if terminal supports ANSI color codes.

    Returns: True if colors should be used, False otherwise
    """
    # Check if stdout is a terminal
    if not hasattr(sys.stdout, "isatty") or not sys.stdout.isatty():
        return False

    # Check TERM environment variable
    term = os.environ.get("TERM", "")
    if term == "dumb":
        return False

    return True


# Color code constants
_USE_COLOR = supports_color()
GREEN = "\033[32m" if _USE_COLOR else ""
YELLOW = "\033[33m" if _USE_COLOR else ""
RED = "\033[31m" if _USE_COLOR else ""
BLUE = "\033[34m" if _USE_COLOR else ""
RESET = "\033[0m" if _USE_COLOR else ""


def format_separator(char: str = "=", width: int = 70) -> str:
    """
    Create a visual divider line.

    Args:
        char: Character to repeat (default: "=")
        width: Width of the separator (default: 70)

    Returns: String of repeated characters
    """
    return char * width


def format_timestamp(dt: Optional[datetime] = None) -> str:
    """
    Format a timestamp for consistent display.

    Args:
        dt: Datetime object (default: now)

    Returns: Formatted timestamp string (YYYY-MM-DD HH:MM:SS)
    """
    if dt is None:
        dt = datetime.now()
    return dt.strftime("%Y-%m-%d %H:%M:%S")


def format_iteration_summary(
    iteration: int,
    spec_format: str,
    task_title: str,
    task_status: str,
    files_added: list[str],
    files_modified: list[str],
    files_deleted: list[str],
    exit_code: int,
    total_tasks: int,
    completed_tasks: int
) -> str:
    """
    Format an iteration summary with structured data.

    Args:
        iteration: Iteration number
        spec_format: "json" or "markdown"
        task_title: Title of the task being worked on
        task_status: "complete" or "incomplete"
        files_added: List of newly added files
        files_modified: List of modified files
        files_deleted: List of deleted files
        exit_code: Exit code of the iteration (0 = success)
        total_tasks: Total number of tasks in spec
        completed_tasks: Number of completed tasks

    Returns: Formatted summary string
    """
    lines = []
    timestamp = format_timestamp()

    # Header separator
    lines.append(format_separator("="))
    lines.append(f"Iteration {iteration} Summary - {timestamp}")
    lines.append(format_separator("="))
    lines.append("")

    # Task being worked on
    lines.append(f"Task: {task_title}")
    lines.append("")

    # Task status (color-coded)
    if task_status == "complete":
        status_text = f"{GREEN}✓ Complete{RESET}"
    else:
        status_text = f"{YELLOW}○ Incomplete{RESET}"
    lines.append(f"Status: {status_text}")
    lines.append("")

    # Files changed
    total_file_changes = len(files_added) + len(files_modified) + len(files_deleted)
    if total_file_changes == 0:
        lines.append("Files changed: None")
    else:
        lines.append(f"Files changed: {total_file_changes}")

        if files_added:
            lines.append(f"  {GREEN}Added ({len(files_added)}){RESET}:")
            for f in files_added[:5]:  # Limit to first 5
                lines.append(f"    + {f}")
            if len(files_added) > 5:
                lines.append(f"    ... and {len(files_added) - 5} more")

        if files_modified:
            lines.append(f"  {BLUE}Modified ({len(files_modified)}){RESET}:")
            for f in files_modified[:5]:  # Limit to first 5
                lines.append(f"    ~ {f}")
            if len(files_modified) > 5:
                lines.append(f"    ... and {len(files_modified) - 5} more")

        if files_deleted:
            lines.append(f"  {RED}Deleted ({len(files_deleted)}){RESET}:")
            for f in files_deleted[:5]:  # Limit to first 5
                lines.append(f"    - {f}")
            if len(files_deleted) > 5:
                lines.append(f"    ... and {len(files_deleted) - 5} more")

    lines.append("")

    # Exit status
    if exit_code == 0:
        exit_status = f"{GREEN}Success{RESET}"
    else:
        exit_status = f"{RED}Failed (exit code: {exit_code}){RESET}"
    lines.append(f"Exit status: {exit_status}")
    lines.append("")

    # Progress
    lines.append(f"Progress: {completed_tasks} of {total_tasks} tasks complete")
    lines.append("")

    # Footer separator
    lines.append(format_separator("="))

    return "\n".join(lines)


# ============================================================================
# Git Change Tracking Utilities
# ============================================================================

def get_git_status() -> dict[str, list[str]]:
    """
    Get current git status and parse into added/modified/deleted file lists.

    Returns: Dict with keys "added", "modified", "deleted" containing file paths
    """
    try:
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True,
            check=True
        )
    except subprocess.CalledProcessError:
        # Not a git repo or git error - return empty lists
        return {"added": [], "modified": [], "deleted": []}

    added = []
    modified = []
    deleted = []

    for line in result.stdout.strip().split("\n"):
        if not line:
            continue

        # Git status --porcelain format:
        # XY FILENAME
        # X = status in index, Y = status in working tree
        # A = added, M = modified, D = deleted, ? = untracked
        status = line[:2]
        filepath = line[3:]

        if "?" in status or "A" in status:
            added.append(filepath)
        elif "M" in status:
            modified.append(filepath)
        elif "D" in status:
            deleted.append(filepath)

    return {"added": added, "modified": modified, "deleted": deleted}


def diff_file_changes(
    before: dict[str, list[str]],
    after: dict[str, list[str]]
) -> dict[str, list[str]]:
    """
    Compute the delta between before/after git status.

    Args:
        before: Git status before iteration
        after: Git status after iteration

    Returns: Dict with keys "added", "modified", "deleted" showing new changes
    """
    # Convert lists to sets for easier comparison
    before_added = set(before["added"])
    before_modified = set(before["modified"])
    before_deleted = set(before["deleted"])

    after_added = set(after["added"])
    after_modified = set(after["modified"])
    after_deleted = set(after["deleted"])

    # Compute differences
    # For newly added files, use simple set subtraction
    newly_added = list(after_added - before_added)
    newly_deleted = list(after_deleted - before_deleted)

    # For modified files, include files that were already modified if they remain modified
    # This catches edits to already-dirty files
    newly_modified = list(after_modified)

    return {
        "added": newly_added,
        "modified": newly_modified,
        "deleted": newly_deleted
    }


# ============================================================================
# Task Extraction Utilities
# ============================================================================

def get_markdown_tasks(spec_file: Path) -> list[tuple[str, str]]:
    """
    Extract all tasks from a Markdown spec with their status.

    Returns: List of (title, status) tuples where status is "complete" or "incomplete"
    """
    content = spec_file.read_text()
    lines = content.split("\n")

    in_tasks = False
    tasks = []
    current_task = None
    prev_was_h3 = False

    for line in lines:
        # Check for entering Step by Step Tasks section
        if line.startswith("## Step by Step Tasks"):
            in_tasks = True
            continue

        # Check for exiting section (hit another h2)
        if in_tasks and line.startswith("## ") and not line.startswith("## Step by Step Tasks"):
            break

        if in_tasks:
            if line.startswith("### "):
                # Extract task title (remove ### prefix)
                title = line[4:].strip()
                current_task = title
                tasks.append((title, "incomplete"))
                prev_was_h3 = True
            elif prev_was_h3:
                # Check if this line marks completion
                if "**Status:**" in line and "complete" in line.lower():
                    # Update the last task's status
                    if tasks:
                        tasks[-1] = (tasks[-1][0], "complete")
                prev_was_h3 = False

    return tasks


def get_json_stories(spec_file: Path) -> list[tuple[str, str, str]]:
    """
    Extract all stories from a JSON spec with their status.

    Returns: List of (story_id, title, status) tuples
    """
    try:
        content = spec_file.read_text()
        data = json.loads(content)
    except (json.JSONDecodeError, OSError):
        return []

    stories = data.get("stories", [])
    result = []

    for story in stories:
        story_id = story.get("id", "")
        title = story.get("title", "Untitled")
        status = story.get("status", "").lower()

        # Normalize status
        if status in ("complete", "done"):
            normalized_status = "complete"
        else:
            normalized_status = "incomplete"

        result.append((story_id, title, normalized_status))

    return result


def find_current_task(tasks: list[tuple]) -> Optional[tuple]:
    """
    Find the first incomplete task from a list of tasks.

    Args:
        tasks: List of task tuples (format depends on spec type)
               For markdown: (title, status)
               For json: (story_id, title, status)

    Returns: The first incomplete task tuple, or None if all complete
    """
    for task in tasks:
        # Status is always the last element in the tuple
        status = task[-1]
        if status == "incomplete":
            return task

    return None


def generate_iteration_summary(
    iteration: int,
    spec_format: str,
    spec_file: Path,
    file_changes: dict[str, list[str]],
    exit_code: int
) -> str:
    """
    Generate a complete iteration summary by gathering task info and formatting it.

    Args:
        iteration: Iteration number
        spec_format: "json" or "markdown"
        spec_file: Path to spec file
        file_changes: Dict with "added", "modified", "deleted" file lists
        exit_code: Exit code of the iteration

    Returns: Formatted summary string
    """
    # Get all tasks
    if spec_format == "json":
        tasks = get_json_stories(spec_file)
    else:
        tasks = get_markdown_tasks(spec_file)

    # Calculate progress
    total_tasks = len(tasks)
    completed_tasks = sum(1 for task in tasks if task[-1] == "complete")

    # Find current task (the one that was just worked on)
    # Prefer the first incomplete task if it exists, as it represents current work
    # Otherwise fall back to the last completed task
    current_task = find_current_task(tasks)
    if current_task:
        task_status = "incomplete"
    else:
        # No incomplete tasks - find the last completed task
        for task in reversed(tasks):
            if task[-1] == "complete":
                current_task = task
                task_status = "complete"
                break

    # Extract task title based on format
    if current_task:
        if spec_format == "json":
            # Format: (story_id, title, status)
            task_title = current_task[1]
        else:
            # Format: (title, status)
            task_title = current_task[0]
    else:
        task_title = "Unknown task"

    # Format and return summary
    return format_iteration_summary(
        iteration=iteration,
        spec_format=spec_format,
        task_title=task_title,
        task_status=task_status,
        files_added=file_changes["added"],
        files_modified=file_changes["modified"],
        files_deleted=file_changes["deleted"],
        exit_code=exit_code,
        total_tasks=total_tasks,
        completed_tasks=completed_tasks
    )


def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Run a Ralph Wiggum loop on a spec file",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Supports two spec formats:
  1. Markdown specs with "## Step by Step Tasks" section (h3 headers as tasks)
  2. JSON PRD files with a "stories" array (legacy format)

Examples:
    %(prog)s --spec specs/feature.md
    %(prog)s --spec specs/feature.json --max-iterations 10
    %(prog)s --prd legacy.json --completion-promise "ALL DONE"
    %(prog)s --spec specs/feature.md --model claude --log /tmp/ralph.log
        """
    )

    # Create mutually exclusive group for --spec and --prd (both do the same thing)
    spec_group = parser.add_mutually_exclusive_group(required=True)
    spec_group.add_argument(
        "--spec",
        dest="spec_file",
        help="Path to spec file (Markdown or JSON)"
    )
    spec_group.add_argument(
        "--prd",
        dest="spec_file",
        help="Path to spec file (legacy alias for --spec)"
    )

    parser.add_argument(
        "--max-iterations",
        type=int,
        default=0,
        help="Maximum iterations (0 = unlimited, default: 0)"
    )
    parser.add_argument(
        "--completion-promise",
        default="TASK COMPLETE",
        help='String to detect completion (default: "TASK COMPLETE")'
    )
    parser.add_argument(
        "--summary-only",
        action="store_true",
        help="Show only iteration summaries, suppress verbose Claude output"
    )
    parser.add_argument(
        "--no-summaries",
        action="store_true",
        help="Disable iteration summaries (for backwards compatibility)"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="claude",
        help="Model to use for agent invocations (default: claude)"
    )
    parser.add_argument(
        "--log",
        "--log-file",
        dest="log_file",
        type=str,
        default=None,
        help="Path to log file for detailed execution capture (optional)"
    )

    args = parser.parse_args()

    # Validate spec file exists
    spec_path = Path(args.spec_file)
    if not spec_path.exists():
        parser.error(f"Spec file not found: {args.spec_file}")

    if not spec_path.is_file():
        parser.error(f"Spec path is not a file: {args.spec_file}")

    return args


def detect_spec_format(spec_file: Path) -> str:
    """
    Detect spec format based on file extension and content.

    Returns: "json" or "markdown"
    Raises: ValueError if format cannot be determined
    """
    # Check file extension first
    suffix = spec_file.suffix.lower()

    if suffix == ".json":
        return "json"
    elif suffix == ".md":
        return "markdown"

    # Try to auto-detect based on content
    content = spec_file.read_text()

    # Try parsing as JSON
    try:
        data = json.loads(content)
        if isinstance(data, dict) and "stories" in data:
            return "json"
    except json.JSONDecodeError:
        pass

    # Check for markdown markers
    if re.search(r"^## Step by Step Tasks", content, re.MULTILINE):
        return "markdown"

    raise ValueError(
        f"Could not detect spec format for: {spec_file}\n"
        "Use .json or .md extension, or ensure the file contains "
        "a 'stories' array (JSON) or '## Step by Step Tasks' section (Markdown)."
    )


def validate_json_spec(spec_file: Path) -> None:
    """
    Validate that a JSON spec file has a 'stories' array.

    Raises: ValueError if validation fails
    """
    try:
        content = spec_file.read_text()
        data = json.loads(content)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in spec file: {e}")

    if not isinstance(data, dict):
        raise ValueError("JSON spec must be an object with a 'stories' array")

    if "stories" not in data:
        raise ValueError("JSON spec must have a 'stories' array")

    if not isinstance(data["stories"], list):
        raise ValueError("'stories' must be an array")


def validate_markdown_spec(spec_file: Path) -> None:
    """
    Validate that a Markdown spec file has a '## Step by Step Tasks' section.

    Raises: ValueError if validation fails
    """
    content = spec_file.read_text()

    if not re.search(r"^## Step by Step Tasks", content, re.MULTILINE):
        raise ValueError(
            f"Markdown spec must have '## Step by Step Tasks' section: {spec_file}"
        )


def initialize_progress_file(spec_file: Path) -> Path:
    """
    Initialize progress file if it doesn't exist.

    Returns: Path to progress file
    """
    spec_name = spec_file.stem  # basename without extension
    progress_file = Path(f"{spec_name}-progress.txt")

    if not progress_file.exists():
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        content = f"""# Progress Log for {spec_name}
# Started: {timestamp}

"""
        progress_file.write_text(content)

    return progress_file


def check_json_completion(spec_file: Path) -> int:
    """
    Count incomplete stories in a JSON spec.

    Returns: Number of pending (incomplete) stories
    """
    try:
        content = spec_file.read_text()
        data = json.loads(content)
    except (json.JSONDecodeError, OSError) as e:
        log("ERROR", f"Failed to parse spec file: {e}")
        sys.exit(1)

    stories = data.get("stories", [])
    pending = 0

    for story in stories:
        status = story.get("status", "").lower()
        if status not in ("complete", "done"):
            pending += 1

    return pending


def check_markdown_completion(spec_file: Path) -> int:
    """
    Count incomplete tasks in a Markdown spec.

    Looks for h3 headers in the '## Step by Step Tasks' section
    that are NOT followed by '**Status:** complete'.

    Returns: Number of pending (incomplete) tasks
    """
    content = spec_file.read_text()
    lines = content.split("\n")

    in_tasks = False
    pending = 0
    prev_was_h3 = False

    for line in lines:
        # Check for entering Step by Step Tasks section
        if line.startswith("## Step by Step Tasks"):
            in_tasks = True
            continue

        # Check for exiting section (hit another h2)
        if in_tasks and line.startswith("## ") and not line.startswith("## Step by Step Tasks"):
            break

        if in_tasks:
            if line.startswith("### "):
                # Found a task header, assume incomplete until we check next line
                prev_was_h3 = True
                pending += 1
            elif prev_was_h3:
                # Check if this line marks completion
                if "**Status:**" in line and "complete" in line.lower():
                    pending -= 1
                prev_was_h3 = False

    return pending


def generate_json_prompt(spec_path: str, progress_file: Path, completion_promise: str) -> str:
    """Generate the iteration prompt for JSON format specs."""
    return f"""# Ralph Wiggum Loop - Iteration Prompt

You are an autonomous AI agent working through a spec to complete all stories.

## Your Task

1. Read `{spec_path}` to understand the stories and their status
2. Read `{progress_file}` to understand what has been learned in previous iterations
3. Select the highest-priority incomplete story that is not blocked
4. Implement the story completely
5. Update `{spec_path}` to mark the story with `"status": "complete"` when done
6. Append learnings to `{progress_file}`

## Important Rules

- **One story per iteration**: Focus on completing one story fully before moving to the next
- **Update the spec file**: When a story is complete, set `"status": "complete"`
- **Append to progress file**: Document what you did, any issues encountered, and learnings
- **Stay focused**: Only work on stories in this spec, ignore other specs
- **Completion promise**: When all stories are complete, output exactly: `{completion_promise}`

## Story Priority Order

1. First, complete all P0 stories (highest priority)
2. Then, complete P1 stories
3. Then, complete P2 stories
4. Respect `blocked_by` dependencies - don't start a story until its dependencies are complete

## Verification

Before marking a story as complete:
- All acceptance criteria met
- Code compiles/lints without errors
- No obvious errors in the implementation

## Begin

Read `{spec_path}` and `{progress_file}` now, then implement the next incomplete story."""


def generate_markdown_prompt(spec_path: str, progress_file: Path, completion_promise: str) -> str:
    """Generate the iteration prompt for Markdown format specs."""
    return f"""# Ralph Wiggum Loop - Iteration Prompt

You are an autonomous AI agent working through a feature spec to complete all tasks.

## Your Task

1. Read `{spec_path}` to understand the feature and the Step by Step Tasks
2. Read `{progress_file}` to understand what has been done in previous iterations
3. Find the first incomplete task (a task without `**Status:** complete` after its heading)
4. Implement that task completely
5. Update `{spec_path}` to mark the task as complete by adding `**Status:** complete` on the line after the h3 heading
6. Append learnings to `{progress_file}`

## Important Rules

- **One task per iteration**: Focus on completing one task fully before moving to the next
- **Execute tasks in order**: Work through tasks from top to bottom as listed in the spec
- **Mark completion**: After the h3 task heading, add a line: `**Status:** complete`
- **Append to progress file**: Document what you did, any issues encountered, and learnings
- **Stay focused**: Only work on tasks in this spec
- **Completion promise**: When all tasks are complete, output exactly: `{completion_promise}`

## Task Status Format

When a task is incomplete, it looks like:
```
### Step 1: Create the database schema
- Create migrations for users table
- Add indexes
```

When you complete it, add the status line:
```
### Step 1: Create the database schema
**Status:** complete
- Create migrations for users table
- Add indexes
```

## Verification

Before marking a task as complete:
- All bullet points under the task are done
- Code compiles/lints without errors
- No obvious errors in the implementation

## Begin

Read `{spec_path}` and `{progress_file}` now, then implement the next incomplete task."""


def check_completion_promise(output: str, promise: str) -> bool:
    """Check if the completion promise appears in the output."""
    return promise in output


def run_claude_iteration(prompt: str, progress_file: Path, iteration: int, summary_only: bool = False, model: str = "claude", logger: Optional[any] = None) -> tuple[int, str]:
    """
    Run a single Claude iteration with the given prompt.

    Args:
        prompt: The prompt to send to Claude
        progress_file: Path to progress file (unused, kept for compatibility)
        iteration: Iteration number
        summary_only: If True, suppress verbose Claude output (only capture, don't print)
        model: Model to use for invocation (default: "claude")
        logger: Optional StructuredLogger instance for logging

    Returns: (exit_code, output)
    """
    import time
    start_time = time.time()

    cmd = [model, "--dangerously-skip-permissions", "--verbose", "-p", prompt]

    # Log command execution start
    if logger:
        logger.log_command("claude_iteration", cmd, "starting")

    # Create temp file to capture output while also streaming it
    with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False) as tmp:
        tmp_path = tmp.name

    try:
        # Run command with tee-like behavior
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        output_lines = []
        with open(tmp_path, "w") as tmp_file:
            for line in process.stdout:
                # Only print if not in summary-only mode
                if not summary_only:
                    print(line, end="", flush=True)
                tmp_file.write(line)
                output_lines.append(line)

        process.wait()
        output = "".join(output_lines)

        # Log command execution completion
        duration = time.time() - start_time
        if logger:
            logger.log_command("claude_iteration", cmd, "completed",
                             exit_code=process.returncode, output=output, duration=duration)

        return process.returncode, output
    except Exception as e:
        # Log errors
        duration = time.time() - start_time
        if logger:
            logger.log_error("claude_iteration", str(e), {"command": cmd, "duration": duration})
        raise
    finally:
        # Clean up temp file
        try:
            os.unlink(tmp_path)
        except OSError:
            pass


def commit_changes(spec_name: str, iteration: int) -> None:
    """Commit any uncommitted changes with an iteration-specific message."""
    # Check if there are uncommitted changes
    result = subprocess.run(
        ["git", "status", "--porcelain"],
        capture_output=True,
        text=True
    )

    if not result.stdout.strip():
        # No changes to commit
        return

    print("Committing changes...")

    # Stage all changes
    subprocess.run(["git", "add", "-A"], check=False)

    # Create commit message
    commit_message = f"""[{spec_name}] Ralph iteration {iteration}

Co-Authored-By: Claude <noreply@anthropic.com>"""

    # Commit changes (don't fail if commit fails)
    subprocess.run(
        ["git", "commit", "-m", commit_message],
        check=False,
        capture_output=True
    )


def run_ralph_loop(args: argparse.Namespace) -> int:
    """Main Ralph loop logic."""
    spec_file = Path(args.spec_file)
    max_iterations = args.max_iterations
    completion_promise = args.completion_promise
    model = args.model
    log_file = args.log_file

    # Initialize logger if log file is provided
    logger = None
    if log_file and StructuredLogger:
        try:
            logger = StructuredLogger(log_file)
            logger.log_event("initialization", {"message": "Ralph loop starting", "spec_file": str(spec_file)})
        except Exception as e:
            print(f"Warning: Could not initialize logger: {e}")
            print("Continuing without logging...")
            logger = None

    # Detect and validate spec format
    try:
        spec_format = detect_spec_format(spec_file)
    except ValueError as e:
        log("ERROR", str(e))
        return 1

    # Validate spec file
    try:
        if spec_format == "json":
            validate_json_spec(spec_file)
        else:
            validate_markdown_spec(spec_file)
    except ValueError as e:
        log("ERROR", str(e))
        return 1

    # Initialize progress file
    progress_file = initialize_progress_file(spec_file)

    spec_path = str(spec_file)
    spec_name = spec_file.stem

    # Print header
    print("======================================")
    print("  Ralph Wiggum Loop")
    print("======================================")
    print()
    print(f"Spec file:          {spec_path}")
    print(f"Spec format:        {spec_format}")
    print(f"Progress file:      {progress_file}")
    print(f"Max iterations:     {max_iterations}")
    print(f"Completion promise: {completion_promise}")
    print(f"Model:              {model}")
    if log_file:
        print(f"Log file:           {log_file}")
    print()

    # Log configuration
    if logger:
        logger.log_event("configuration", {
            "spec_file": spec_path,
            "spec_format": spec_format,
            "progress_file": str(progress_file),
            "max_iterations": max_iterations,
            "completion_promise": completion_promise,
            "model": model
        })

    iteration = 0

    while True:
        # Check max iterations
        if max_iterations > 0 and iteration >= max_iterations:
            print(f"Max iterations ({max_iterations}) reached.")
            if logger:
                logger.log_event("max_iterations_reached", {"max_iterations": max_iterations})
            break

        iteration += 1

        print("--------------------------------------")
        print(f"  Iteration {iteration} of {max_iterations}")
        print("--------------------------------------")

        # Check if all tasks are complete
        if spec_format == "json":
            pending = check_json_completion(spec_file)
        else:
            pending = check_markdown_completion(spec_file)

        if pending == 0:
            print("All stories complete! Exiting loop.")
            if logger:
                logger.log_event("all_tasks_complete", {"iteration": iteration})
            break

        print(f"Pending stories: {pending}")
        print()

        # Log iteration start
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(progress_file, "a") as f:
            f.write(f"\n### Iteration {iteration} - {timestamp}\n")

        if logger:
            logger.log_iteration_start(iteration, {"pending": pending})

        # Generate format-specific prompt
        if spec_format == "json":
            prompt = generate_json_prompt(spec_path, progress_file, completion_promise)
        else:
            prompt = generate_markdown_prompt(spec_path, progress_file, completion_promise)

        # Capture git status before iteration
        before_files = get_git_status()

        # Run Claude iteration
        print("Running Claude...")
        iteration_start_time = time.time()
        exit_code, output = run_claude_iteration(prompt, progress_file, iteration, args.summary_only, model, logger)
        iteration_duration = time.time() - iteration_start_time

        # Capture git status after iteration
        after_files = get_git_status()

        # Compute file changes during this iteration
        file_changes = diff_file_changes(before_files, after_files)

        if exit_code == 0:
            print("Iteration completed successfully")
            with open(progress_file, "a") as f:
                f.write("Status: Completed\n")

            if logger:
                logger.log_iteration_end(iteration, {"status": "success", "exit_code": exit_code, "duration": iteration_duration})

            # Check for completion promise
            if check_completion_promise(output, completion_promise):
                print()
                print(f"Completion promise detected: '{completion_promise}'")
                if logger:
                    logger.log_event("completion_promise", {"promise": completion_promise, "iteration": iteration})
                break
        else:
            print("Iteration failed")
            with open(progress_file, "a") as f:
                f.write("Status: Failed\n")

            if logger:
                logger.log_iteration_end(iteration, {"status": "failed", "exit_code": exit_code, "duration": iteration_duration})

        # Commit any changes
        commit_changes(spec_name, iteration)

        # Generate and display iteration summary (unless disabled)
        if not args.no_summaries:
            print()
            summary = generate_iteration_summary(
                iteration=iteration,
                spec_format=spec_format,
                spec_file=spec_file,
                file_changes=file_changes,
                exit_code=exit_code
            )
            print(summary)
            print()

            # Append summary to progress file (without color codes)
            # Remove ANSI color codes for file storage
            summary_plain = summary
            for color_code in [GREEN, YELLOW, RED, BLUE, RESET]:
                summary_plain = summary_plain.replace(color_code, "")

            with open(progress_file, "a") as f:
                f.write("\n")
                f.write(summary_plain)
                f.write("\n")

        # Sleep between iterations
        time.sleep(2)

    # Final status
    print()
    print("======================================")
    print("  Ralph Loop Complete")
    print("======================================")

    if spec_format == "json":
        final_pending = check_json_completion(spec_file)
    else:
        final_pending = check_markdown_completion(spec_file)

    if final_pending == 0:
        print("SUCCESS: All stories completed!")
        if logger:
            logger.log_event("ralph_loop_complete", {"status": "success", "total_iterations": iteration})
            logger.close()
        return 0
    else:
        print(f"INCOMPLETE: {final_pending} stories remaining")
        if logger:
            logger.log_event("ralph_loop_complete", {"status": "incomplete", "total_iterations": iteration, "pending": final_pending})
            logger.close()
        return 1


def main() -> int:
    """Main entry point for ralph-loop."""
    args = parse_arguments()
    return run_ralph_loop(args)


if __name__ == "__main__":
    sys.exit(main())
