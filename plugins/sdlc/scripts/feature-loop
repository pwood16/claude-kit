#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.10"
# dependencies = []
# ///
"""
feature-loop - Automated SDLC feature planning, implementation, and review loop.

This script chains together the SDLC workflow:
1. Create a plan using /sdlc:feature
2. Implement it using ralph-loop (iterative task completion)
3. Run code review with acr
4. Triage findings (fix real issues, comment on false positives)
5. Repeat review until clean (LGTM)

Usage:
    feature-loop "Add user authentication"
    feature-loop --prompt "Add search functionality" --max-review-iterations 3
    feature-loop "Add feature" --max-implement-iterations 10
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Optional, TypedDict

# Import StructuredLogger for log capture
try:
    from lib.logger import StructuredLogger
except ImportError:
    # Add lib directory to path if running as script
    script_dir = Path(__file__).parent.parent / "lib"
    if script_dir.exists():
        sys.path.insert(0, str(script_dir))
        from logger import StructuredLogger
    else:
        # Logger not available - will be None when log_file is requested
        StructuredLogger = None


# Checkpoint constants
CHECKPOINT_VERSION = "1"
DEFAULT_STATE_FILE = ".feature-loop-state.json"


class CheckpointState(TypedDict):
    """State structure for feature-loop checkpoint/resume functionality."""
    version: str  # Schema version for forward compatibility
    feature_prompt: str  # Original feature description
    plan_path: str  # Path to spec file
    phase: str  # One of: "planning", "implementing", "reviewing", "complete"
    review_iteration: int  # Current review iteration, 0 if not in review
    started_at: str  # ISO timestamp
    updated_at: str  # ISO timestamp
    config: dict[str, Any]  # Configuration snapshot


class FeatureLoopConfig(TypedDict, total=False):
    """Configuration structure for feature_loop section in .claude-kit."""
    model: str  # Model to use for agent invocations (default: "claude")
    max_implement_iterations: int  # Max implementation iterations (0 = unlimited)
    max_review_iterations: int  # Max review iterations
    skip_review: bool  # Skip review phase
    verbose: bool  # Enable verbose logging


class RalphLoopConfig(TypedDict, total=False):
    """Configuration structure for ralph_loop section in .claude-kit."""
    model: str  # Model to use for agent invocations (default: "claude")


class ACRConfig(TypedDict, total=False):
    """Configuration structure for acr section in .claude-kit."""
    num_reviewers: int  # Number of reviewers for ACR (0 = use acr default)
    config: dict[str, Any]  # Snapshot of configuration used


def log(step: str, message: str) -> None:
    """Log a message with timestamp and step indicator."""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] [{step}] {message}", flush=True)


def run_command(cmd: list[str], capture_output: bool = True, interactive: bool = False) -> subprocess.CompletedProcess:
    """Run a command and return the result.

    Args:
        cmd: Command and arguments to run
        capture_output: If True, capture stdout/stderr. If False, show output in terminal.
        interactive: If True, allow stdin from terminal. If False, detach stdin to prevent prompts.
    """
    stdin = None if interactive else subprocess.DEVNULL
    if capture_output:
        return subprocess.run(cmd, capture_output=True, text=True, stdin=stdin)
    else:
        return subprocess.run(cmd, text=True, stdin=stdin)


def check_command_exists(cmd: str) -> bool:
    """Check if a command exists in PATH."""
    result = subprocess.run(["which", cmd], capture_output=True, text=True)
    return result.returncode == 0


def find_git_root() -> Optional[Path]:
    """Find the git root directory, if in a git repository."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            capture_output=True,
            text=True,
            check=False
        )
        if result.returncode == 0:
            return Path(result.stdout.strip())
    except Exception:
        pass
    return None


def discover_config_files(verbose: bool = False) -> list[Path]:
    """
    Discover .claude-kit configuration files in order of precedence (lowest to highest).

    Search order:
    1. User home directory (~/.claude-kit)
    2. Git root directory (./.claude-kit)
    3. Current working directory (./.claude-kit)

    Returns list of existing config file paths in precedence order (lowest first).
    """
    config_files = []

    # 1. User home directory (lowest precedence)
    home_config = Path.home() / ".claude-kit"
    if home_config.exists():
        config_files.append(home_config)
        if verbose:
            log("CONFIG", f"Found config: {home_config}")

    # 2. Git root directory (if in a git repo and different from cwd)
    git_root = find_git_root()
    if git_root:
        git_config = git_root / ".claude-kit"
        cwd = Path.cwd()
        if git_config.exists() and git_root != cwd:
            config_files.append(git_config)
            if verbose:
                log("CONFIG", f"Found config: {git_config}")

    # 3. Current working directory (highest precedence)
    cwd_config = Path.cwd() / ".claude-kit"
    if cwd_config.exists():
        config_files.append(cwd_config)
        if verbose:
            log("CONFIG", f"Found config: {cwd_config}")

    return config_files


def load_config_file(config_path: Path, verbose: bool = False) -> dict[str, Any]:
    """
    Load and parse a configuration file.

    NOTE: Docstring says "returns empty dict on error" but implementation calls sys.exit(1).
    This is intentional - the function used to return empty dict but was changed to exit on
    errors because config errors should be fatal. The docstring is outdated but the behavior
    is correct.

    Returns the parsed configuration or exits on error.
    """
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)

        if verbose:
            log("CONFIG", f"Loaded config from: {config_path}")

        return config
    except json.JSONDecodeError as e:
        log("ERROR", f"Invalid JSON in config file {config_path}: {e}")
        sys.exit(1)
    except Exception as e:
        log("ERROR", f"Error reading config file {config_path}: {e}")
        sys.exit(1)


def merge_configs(configs: list[dict[str, Any]]) -> dict[str, Any]:
    """
    Merge multiple configuration dictionaries with later configs taking precedence.

    Performs a deep merge of nested dictionaries.
    """
    result = {}

    for config in configs:
        for key, value in config.items():
            if isinstance(value, dict) and key in result and isinstance(result[key], dict):
                # Deep merge for nested dicts
                result[key] = {**result[key], **value}
            else:
                result[key] = value

    return result


def validate_config(config: dict[str, Any]) -> tuple[FeatureLoopConfig, ACRConfig, RalphLoopConfig]:
    """
    Validate configuration values and return validated settings.

    Returns a tuple of (feature_loop_config, acr_config, ralph_loop_config) or exits on validation error.
    """
    feature_loop_config = config.get("feature_loop", {})
    acr_config = config.get("acr", {})
    ralph_loop_config = config.get("ralph_loop", {})

    # Validate that feature_loop is a dict/object, not array or other type
    if not isinstance(feature_loop_config, dict):
        log("ERROR", f"feature_loop config must be a JSON object (dict), got: {type(feature_loop_config).__name__}")
        sys.exit(1)

    # Validate that acr is a dict/object, not array or other type
    if not isinstance(acr_config, dict):
        log("ERROR", f"acr config must be a JSON object (dict), got: {type(acr_config).__name__}")
        sys.exit(1)

    # Validate that ralph_loop is a dict/object, not array or other type
    if not isinstance(ralph_loop_config, dict):
        log("ERROR", f"ralph_loop config must be a JSON object (dict), got: {type(ralph_loop_config).__name__}")
        sys.exit(1)

    # Validate max_review_iterations
    if "max_review_iterations" in feature_loop_config:
        value = feature_loop_config["max_review_iterations"]
        if not isinstance(value, int) or value < 1:
            log("ERROR", f"max_review_iterations must be a positive integer, got: {value}")
            sys.exit(1)

    # Validate skip_review
    if "skip_review" in feature_loop_config:
        value = feature_loop_config["skip_review"]
        if not isinstance(value, bool):
            log("ERROR", f"skip_review must be a boolean (true/false), got: {value}")
            sys.exit(1)

    # Validate verbose
    if "verbose" in feature_loop_config:
        value = feature_loop_config["verbose"]
        if not isinstance(value, bool):
            log("ERROR", f"verbose must be a boolean (true/false), got: {value}")
            sys.exit(1)

    # Validate max_implement_iterations
    # NOTE: Allows 0 (meaning unlimited), unlike review-loop which requires >= 1.
    # This is intentional - different scripts have different semantics for iteration limits.
    if "max_implement_iterations" in feature_loop_config:
        value = feature_loop_config["max_implement_iterations"]
        if not isinstance(value, int) or value < 0:
            log("ERROR", f"max_implement_iterations must be a non-negative integer, got: {value}")
            sys.exit(1)

    # Validate model
    if "model" in feature_loop_config:
        value = feature_loop_config["model"]
        if not isinstance(value, str) or not value.strip():
            log("ERROR", f"feature_loop.model must be a non-empty string, got: {value}")
            sys.exit(1)

    # Validate ralph_loop.model
    if "model" in ralph_loop_config:
        value = ralph_loop_config["model"]
        if not isinstance(value, str) or not value.strip():
            log("ERROR", f"ralph_loop.model must be a non-empty string, got: {value}")
            sys.exit(1)

    # Validate acr.num_reviewers
    # NOTE: 0 is a valid value meaning "use acr default"
    if "num_reviewers" in acr_config:
        value = acr_config["num_reviewers"]
        if not isinstance(value, int) or value < 0:
            log("ERROR", f"acr.num_reviewers must be a non-negative integer (0=default), got: {value}")
            sys.exit(1)

    return feature_loop_config, acr_config, ralph_loop_config


def load_configuration(verbose: bool = False) -> tuple[FeatureLoopConfig, ACRConfig, RalphLoopConfig]:
    """
    Load and merge configuration from all discovered config files.

    Returns a tuple of (feature_loop_config, acr_config, ralph_loop_config).
    Precedence: defaults < user home < git root < current dir < CLI args
    """
    config_files = discover_config_files(verbose)

    if not config_files and verbose:
        log("CONFIG", "No configuration files found, using defaults")

    # Load all config files
    configs = [load_config_file(path, verbose) for path in config_files]

    # Merge configurations
    merged_config = merge_configs(configs)

    # Validate and extract settings
    feature_loop_config, acr_config, ralph_loop_config = validate_config(merged_config)

    return feature_loop_config, acr_config, ralph_loop_config


def get_state_file_path(feature_prompt: str = "") -> Path:
    """
    Generate path for checkpoint state file.

    For now, uses a fixed filename in the current directory.
    Future enhancement: could generate deterministic filename based on feature_prompt hash.
    """
    return Path.cwd() / DEFAULT_STATE_FILE


def save_checkpoint(state: CheckpointState, verbose: bool = False) -> None:
    """
    Save checkpoint state to disk atomically.

    Uses atomic write (write to temp, then rename) to prevent corruption.
    """
    state_file = get_state_file_path(state["feature_prompt"])

    # Update timestamp
    state["updated_at"] = datetime.now().isoformat()

    try:
        # Write to temp file first for atomic operation
        temp_file = state_file.with_suffix(".tmp")
        with open(temp_file, 'w') as f:
            json.dump(state, f, indent=2)

        # Atomic rename
        temp_file.rename(state_file)

        if verbose:
            log("CHECKPOINT", f"Saved checkpoint: {state_file} (phase: {state['phase']})")
    except Exception as e:
        log("ERROR", f"Failed to save checkpoint: {e}")
        # Don't exit - checkpoint save failure shouldn't stop the workflow


def load_checkpoint(state_file: Path, verbose: bool = False) -> Optional[CheckpointState]:
    """
    Load checkpoint state from disk.

    Returns None if file doesn't exist or is invalid.
    Validates version compatibility.
    """
    if not state_file.exists():
        return None

    try:
        with open(state_file, 'r') as f:
            state = json.load(f)

        # Validate version
        if state.get("version") != CHECKPOINT_VERSION:
            log("WARNING", f"Checkpoint version mismatch: expected {CHECKPOINT_VERSION}, got {state.get('version')}")
            log("WARNING", "Consider starting fresh with --no-resume")
            return None

        # Validate required fields
        required_fields = ["version", "feature_prompt", "plan_path", "phase", "review_iteration", "started_at", "updated_at", "config"]
        for field in required_fields:
            if field not in state:
                log("WARNING", f"Invalid checkpoint: missing field '{field}'")
                return None

        if verbose:
            log("CHECKPOINT", f"Loaded checkpoint: {state_file} (phase: {state['phase']})")

        return state
    except json.JSONDecodeError as e:
        log("WARNING", f"Corrupted checkpoint file: {e}")
        return None
    except Exception as e:
        log("WARNING", f"Failed to load checkpoint: {e}")
        return None


def clear_checkpoint(state_file: Path, verbose: bool = False) -> None:
    """
    Remove checkpoint state file.

    Safe to call if file doesn't exist.
    """
    if state_file.exists():
        try:
            state_file.unlink()
            if verbose:
                log("CHECKPOINT", f"Cleared checkpoint: {state_file}")
        except Exception as e:
            log("WARNING", f"Failed to clear checkpoint: {e}")


def run_feature_plan(feature_prompt: str, verbose: bool = False, logger: Optional[Any] = None, model: str = "claude") -> str:
    """
    Run /sdlc:feature to create a plan.

    Returns the path to the created plan file.
    """
    log("PLAN", f"Creating feature plan for: {feature_prompt}")

    if logger:
        logger.log_event("planning_start", {
            "feature_prompt": feature_prompt,
            "model": model,
        })

    prompt = f"/sdlc:feature {feature_prompt}"
    cmd = [model, "--dangerously-skip-permissions", "-p", prompt]

    if verbose:
        log("PLAN", f"Running: {' '.join(cmd)}")

    if logger:
        logger.log_command(cmd, start=True)

    import time
    start_time = time.time()

    result = run_command(cmd)

    duration = time.time() - start_time

    if logger:
        logger.log_command(
            cmd,
            output=result.stdout + result.stderr,
            exit_code=result.returncode,
            duration=duration
        )

    if result.returncode != 0:
        log("PLAN", f"Error creating plan: {result.stderr}")
        if logger:
            logger.log_error("planning_failed", error=result.stderr, exit_code=result.returncode)
        sys.exit(1)

    output = result.stdout + result.stderr

    # Extract the plan file path from the output
    # Look for patterns like specs/issue-*.md
    plan_patterns = [
        r'specs/issue-[\w-]+\.md',
        r'`(specs/issue-[\w-]+\.md)`',
        r'created.*?(specs/issue-[\w-]+\.md)',
    ]

    plan_path = None
    for pattern in plan_patterns:
        match = re.search(pattern, output, re.IGNORECASE)
        if match:
            # Get the first group if it exists, otherwise the whole match
            plan_path = match.group(1) if match.lastindex else match.group(0)
            break

    if not plan_path:
        # Try to find any recently created spec file
        # Resolve relative to git root if available, otherwise CWD
        git_root = find_git_root()
        base_dir = git_root if git_root else Path.cwd()
        specs_dir = base_dir / "specs"
        if specs_dir.exists():
            spec_files = sorted(specs_dir.glob("issue-*.md"), key=lambda p: p.stat().st_mtime, reverse=True)
            if spec_files:
                # Return path relative to CWD for compatibility
                try:
                    plan_path = str(spec_files[0].relative_to(Path.cwd()))
                except ValueError:
                    # If not relative to CWD, use absolute path
                    plan_path = str(spec_files[0])

    # Resolve plan_path for existence check
    if plan_path:
        # If relative path, try relative to git root first, then CWD
        check_path = Path(plan_path)
        if not check_path.is_absolute():
            git_root = find_git_root()
            if git_root:
                git_relative = git_root / plan_path
                if git_relative.exists():
                    check_path = git_relative
                    # Normalize plan_path to the resolved absolute path
                    plan_path = str(check_path)
        plan_exists = check_path.exists()
    else:
        plan_exists = False

    if not plan_path or not plan_exists:
        log("PLAN", "Failed to find created plan file")
        log("PLAN", f"Output was:\n{output}")
        if logger:
            logger.log_error("planning_failed", error="Failed to find created plan file", output=output)
        sys.exit(1)

    log("PLAN", f"Plan created: {plan_path}")
    if logger:
        logger.log_event("planning_complete", {
            "plan_path": plan_path,
            "duration": duration,
        })
    return plan_path


def find_ralph_loop_script() -> Optional[Path]:
    """Find the ralph-loop script in the spawn plugin."""
    # Look for ralph-loop relative to this script's location
    script_dir = Path(__file__).parent

    # Try relative path from sdlc plugin to spawn plugin
    spawn_script = script_dir.parent.parent / "spawn" / "scripts" / "ralph-loop"
    if spawn_script.exists():
        return spawn_script

    # Try git root
    git_root = find_git_root()
    if git_root:
        spawn_script = git_root / "plugins" / "spawn" / "scripts" / "ralph-loop"
        if spawn_script.exists():
            return spawn_script

    # Try PATH
    result = subprocess.run(["which", "ralph-loop"], capture_output=True, text=True)
    if result.returncode == 0:
        return Path(result.stdout.strip())

    return None


def run_implement(plan_path: str, verbose: bool = False, max_iterations: int = 0, logger: Optional[Any] = None, model: str = "claude", log_file: Optional[str] = None) -> None:
    """Run ralph-loop to implement the plan iteratively."""
    log("IMPLEMENT", f"Implementing plan with ralph-loop: {plan_path}")

    if logger:
        logger.log_event("implementation_start", {
            "plan_path": plan_path,
            "max_iterations": max_iterations,
            "model": model,
        })

    ralph_script = find_ralph_loop_script()
    if not ralph_script:
        log("ERROR", "Could not find ralph-loop script. Ensure spawn plugin is installed.")
        if logger:
            logger.log_error("implementation_failed", error="Ralph-loop script not found")
        sys.exit(1)

    cmd = [str(ralph_script), "--spec", plan_path, "--model", model]
    if max_iterations > 0:
        cmd.extend(["--max-iterations", str(max_iterations)])
    if log_file:
        # Pass a separate log file for ralph-loop (append "-ralph" to distinguish)
        from pathlib import Path
        log_path = Path(log_file)
        ralph_log = str(log_path.with_stem(log_path.stem + "-ralph"))
        cmd.extend(["--log", ralph_log])

    if verbose:
        log("IMPLEMENT", f"Running: {' '.join(cmd)}")

    if logger:
        logger.log_command(cmd, start=True)

    # Run ralph-loop without capturing output so user sees progress
    import time
    start_time = time.time()

    result = run_command(cmd, capture_output=False)

    duration = time.time() - start_time

    if logger:
        logger.log_event("implementation_complete", {
            "exit_code": result.returncode,
            "duration": duration,
        })

    if result.returncode != 0:
        log("IMPLEMENT", f"Ralph loop exited with code: {result.returncode}")
        # Don't exit - ralph-loop may exit non-zero if not all tasks complete
        # The review loop will catch any issues

    log("IMPLEMENT", "Implementation phase complete")


def supports_color() -> bool:
    """
    Detect if the terminal supports ANSI color codes.

    Returns False if:
    - stdout is not a TTY (output is piped/redirected)
    - TERM environment variable is "dumb"
    - NO_COLOR environment variable is set

    Returns True otherwise.
    """
    import os
    import sys

    # Check if NO_COLOR is set
    if os.environ.get("NO_COLOR"):
        return False

    # Check if stdout is a TTY
    if not sys.stdout.isatty():
        return False

    # Check TERM environment variable
    term = os.environ.get("TERM", "")
    if term == "dumb":
        return False

    return True


# Color constants - evaluated once at module load time
_COLOR_SUPPORT = supports_color()
GREEN = "\033[32m" if _COLOR_SUPPORT else ""
YELLOW = "\033[33m" if _COLOR_SUPPORT else ""
RED = "\033[31m" if _COLOR_SUPPORT else ""
BLUE = "\033[34m" if _COLOR_SUPPORT else ""
RESET = "\033[0m" if _COLOR_SUPPORT else ""


def format_separator(char: str = "=", width: int = 70) -> str:
    """
    Create a visual separator line.

    Args:
        char: Character to use for the separator (default: "=")
        width: Width of the separator line (default: 70)

    Returns:
        A string of repeated characters of the specified width
    """
    return char * width


def format_timestamp() -> str:
    """
    Format current timestamp for display in summaries.

    Returns:
        Formatted timestamp string (e.g., "2024-01-15 14:30:22")
    """
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def parse_acr_output(review_output: str, exit_code: int) -> dict[str, Any]:
    """
    Parse ACR review output to extract structured findings data.

    Args:
        review_output: Raw output from acr command (stdout + stderr)
        exit_code: Exit code from acr command (0=LGTM, 1=findings, 2=error)

    Returns:
        Dictionary with parsed data:
        - total_findings: int count of findings
        - files_with_issues: list of file paths that have issues
        - findings_per_file: dict mapping file path to count
        - finding_types: list of finding categories/types (if detectable)
        - is_lgtm: bool, True if exit code 0
        - raw_output: original output for fallback
    """
    parsed_data = {
        "total_findings": 0,
        "files_with_issues": [],
        "findings_per_file": {},
        "finding_types": [],
        "is_lgtm": exit_code == 0,
        "raw_output": review_output
    }

    # If LGTM, return early
    if exit_code == 0:
        return parsed_data

    # Parse file paths from output
    # Common patterns:
    # - "path/to/file.py:123: finding description"
    # - "path/to/file.py line 123: finding"
    # - "File: path/to/file.py"
    file_path_patterns = [
        r'^([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+):\d+:',  # file.py:123:
        r'^([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+)\s+line\s+\d+:',  # file.py line 123:
        r'File:\s*([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+)',  # File: file.py
        r'^\s*([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+)\s*$',  # file.py on its own line
    ]

    files_found = set()
    file_finding_counts = {}

    # NOTE: Using re.MULTILINE on already-split lines is harmless though not necessary.
    # The flag changes ^ and $ behavior but since we're matching individual lines after split(),
    # it has no effect. Removing it would be an optimization but keeping it doesn't hurt.
    for line in review_output.split('\n'):
        for pattern in file_path_patterns:
            match = re.search(pattern, line, re.MULTILINE)
            if match:
                file_path = match.group(1)
                files_found.add(file_path)
                file_finding_counts[file_path] = file_finding_counts.get(file_path, 0) + 1
                break

    # Try to extract finding types/categories
    # Look for common patterns like "Error:", "Warning:", "Security:", etc.
    finding_type_patterns = [
        r'(Error|Warning|Info|Security|Performance|Style|Bug|Critical):\s',
        r'\[(Error|Warning|Info|Security|Performance|Style|Bug|Critical)\]',
    ]

    finding_types_found = set()
    for line in review_output.split('\n'):
        for pattern in finding_type_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                finding_types_found.add(match.group(1).capitalize())

    # Estimate total findings
    # Strategy: count lines that look like findings
    # Heuristic: lines with file:line: pattern or starting with "- "
    finding_line_patterns = [
        r'^[a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+:\d+:',  # file.py:123:
        r'^\s*-\s+\w',  # - Finding description
        r'^\s*\d+\.\s+\w',  # 1. Finding description
    ]

    finding_count = 0
    for line in review_output.split('\n'):
        for pattern in finding_line_patterns:
            if re.match(pattern, line):
                finding_count += 1
                break

    # If no findings detected via patterns but exit code is 1, set to unknown
    if finding_count == 0 and exit_code == 1:
        # Check if output contains words like "finding", "issue", "problem"
        if re.search(r'\b(finding|issue|problem|error|warning)s?\b', review_output, re.IGNORECASE):
            finding_count = len(files_found) if files_found else 1

    parsed_data["total_findings"] = finding_count
    parsed_data["files_with_issues"] = sorted(list(files_found))
    parsed_data["findings_per_file"] = file_finding_counts
    parsed_data["finding_types"] = sorted(list(finding_types_found))

    return parsed_data


def format_acr_summary(parsed_data: dict[str, Any], iteration: int) -> str:
    """
    Format ACR review summary for display.

    Args:
        parsed_data: Dictionary returned by parse_acr_output()
        iteration: Current review iteration number

    Returns:
        Formatted summary string with colors and visual separators
    """
    lines = []
    timestamp = format_timestamp()

    # Header with separator
    lines.append(format_separator("=", 70))
    lines.append(f"Code Review Summary - Iteration {iteration} - {timestamp}")
    lines.append(format_separator("=", 70))
    lines.append("")

    # Total findings count with color coding
    total_findings = parsed_data["total_findings"]
    if parsed_data["is_lgtm"]:
        findings_line = f"{GREEN}Total Findings: 0 (LGTM ✓){RESET}"
        status_line = f"{GREEN}Status: LGTM - Code is ready!{RESET}"
    else:
        if total_findings == 0:
            findings_color = YELLOW
            findings_text = "Unknown number of findings"
        elif total_findings <= 5:
            findings_color = YELLOW
            findings_text = f"{total_findings} finding(s)"
        else:
            findings_color = RED
            findings_text = f"{total_findings} finding(s)"

        findings_line = f"{findings_color}Total Findings: {findings_text}{RESET}"
        status_line = f"{YELLOW}Status: Issues found - proceeding to triage{RESET}"

    lines.append(findings_line)
    lines.append("")

    # Files with issues
    if parsed_data["files_with_issues"]:
        lines.append("Affected Files:")
        files_to_show = parsed_data["files_with_issues"][:10]  # Limit to first 10 files
        for file_path in files_to_show:
            count = parsed_data["findings_per_file"].get(file_path, 0)
            count_text = f" ({count} finding(s))" if count > 0 else ""
            lines.append(f"  {BLUE}- {file_path}{count_text}{RESET}")

        if len(parsed_data["files_with_issues"]) > 10:
            remaining = len(parsed_data["files_with_issues"]) - 10
            lines.append(f"  ... and {remaining} more file(s)")
        lines.append("")

    # Finding types/categories
    if parsed_data["finding_types"]:
        types_str = ", ".join(parsed_data["finding_types"])
        lines.append(f"Finding Categories: {types_str}")
        lines.append("")

    # Status
    lines.append(status_line)
    lines.append("")
    lines.append(format_separator("=", 70))

    return "\n".join(lines)


def get_progress_file_path(spec_file) -> Path:
    """
    Get the path to the progress file for a given spec file.

    Args:
        spec_file: Path to the spec file (str or Path)

    Returns:
        Path to the progress file
    """
    spec_path = Path(spec_file) if isinstance(spec_file, str) else spec_file
    spec_name = spec_path.stem
    return Path(f"{spec_name}-progress.txt")


def get_feature_loop_log_path(feature_prompt: str) -> Path:
    """
    Get the path to the feature-loop log file for a given feature prompt.

    Args:
        feature_prompt: The feature description

    Returns:
        Path to the log file
    """
    # Use a sanitized version of the feature prompt for the log filename
    # (similar to how checkpoint state file is named)
    safe_name = feature_prompt[:50].replace(" ", "-").replace("/", "-")
    return Path(f".feature-loop-{safe_name}.log")


def log_summary_to_file(log_path: Path, summary: str) -> None:
    """
    Append a summary to the feature-loop log file (without color codes).

    Args:
        log_path: Path to log file
        summary: Summary string (may contain ANSI color codes)
    """
    # Strip ANSI color codes for file storage
    summary_plain = summary
    for code in [GREEN, YELLOW, RED, BLUE, RESET]:
        summary_plain = summary_plain.replace(code, "")

    try:
        with open(log_path, "a") as f:
            f.write("\n")
            f.write(summary_plain)
            f.write("\n")
    except Exception as e:
        # Don't fail if logging fails - just skip it
        pass


def parse_implementation_results(spec_file) -> dict[str, Any]:
    """
    Parse implementation results from progress file and spec file.

    Args:
        spec_file: Path to the spec file (str or Path)

    Returns:
        Dict with:
        - iterations_run: Number of iterations in progress file
        - total_tasks: Total number of tasks in spec
        - completed_tasks: Number of completed tasks
        - incomplete_tasks: Number of incomplete tasks
    """
    # Convert to Path if needed
    spec_path = Path(spec_file) if isinstance(spec_file, str) else spec_file

    # Count iterations from progress file
    progress_file = get_progress_file_path(spec_path)
    iterations_run = 0

    if progress_file.exists():
        content = progress_file.read_text()
        # Count "Iteration N Summary" or "### Iteration N" patterns
        import re
        iteration_patterns = [
            r'Iteration\s+\d+\s+Summary',
            r'###\s+Iteration\s+\d+',
        ]
        for pattern in iteration_patterns:
            matches = re.findall(pattern, content)
            if matches:
                iterations_run = max(iterations_run, len(matches))

    # Parse spec file to count tasks
    total_tasks = 0
    completed_tasks = 0

    if spec_path.suffix == ".json":
        # JSON spec format
        import json
        try:
            with spec_path.open() as f:
                spec_data = json.load(f)

            if "stories" in spec_data:
                for story in spec_data["stories"]:
                    total_tasks += 1
                    status = story.get("status", "").lower()
                    if status in ["complete", "done"]:
                        completed_tasks += 1
        except (json.JSONDecodeError, KeyError):
            pass
    else:
        # Markdown spec format
        content = spec_path.read_text()
        lines = content.split("\n")

        in_tasks = False
        for i, line in enumerate(lines):
            # Look for "Step by Step Tasks" or similar header
            if "step by step" in line.lower() and "task" in line.lower():
                in_tasks = True
                continue

            # Stop counting when we hit another h2 section (##) after entering tasks section
            # NOTE: The compound condition is intentional, not overly complex. We need to check that
            # the line starts with "## " AND does NOT contain both "step by step" and "task" to
            # correctly handle the edge case where the section header itself is on multiple lines or
            # when we encounter other h2 sections. This prevents premature exit from the tasks section.
            if in_tasks and line.strip().startswith("## ") and not ("step by step" in line.lower() and "task" in line.lower()):
                break

            if in_tasks:
                # Look for h3 headings (### )
                if line.strip().startswith("### "):
                    total_tasks += 1
                    # Check if next line has "**Status:** complete"
                    if i + 1 < len(lines):
                        next_line = lines[i + 1].strip()
                        if "**status:**" in next_line.lower() and "complete" in next_line.lower():
                            completed_tasks += 1

    incomplete_tasks = total_tasks - completed_tasks

    return {
        "iterations_run": iterations_run,
        "total_tasks": total_tasks,
        "completed_tasks": completed_tasks,
        "incomplete_tasks": incomplete_tasks,
    }


def get_implementation_file_changes(before_commit: str, after_commit: str = "HEAD") -> list[str]:
    """
    Get list of files modified during implementation phase.

    Args:
        before_commit: Git commit/ref before implementation
        after_commit: Git commit/ref after implementation (default: HEAD)

    Returns:
        List of file paths that were modified
    """
    import subprocess

    try:
        result = subprocess.run(
            ["git", "diff", "--name-only", before_commit, after_commit],
            capture_output=True,
            text=True,
            check=True,
        )
        files = [f for f in result.stdout.strip().split("\n") if f]
        return files
    except subprocess.CalledProcessError:
        # Git error - return empty list
        return []


def format_implementation_summary(
    parsed_data: dict[str, Any],
    modified_files: list[str]
) -> str:
    """
    Format implementation phase summary for display.

    Args:
        parsed_data: Parsed implementation results from parse_implementation_results()
        modified_files: List of files modified during implementation

    Returns:
        Formatted summary string with colors and visual separators
    """
    lines = []
    timestamp = format_timestamp()

    # Header with separator
    lines.append("")
    lines.append(format_separator("=", 70))
    lines.append(f"Implementation Phase Complete - {timestamp}")
    lines.append(format_separator("=", 70))
    lines.append("")

    # Iteration count
    iterations = parsed_data["iterations_run"]
    lines.append(f"Total Iterations: {BLUE}{iterations}{RESET}")
    lines.append("")

    # Task completion status
    total = parsed_data["total_tasks"]
    completed = parsed_data["completed_tasks"]
    incomplete = parsed_data["incomplete_tasks"]

    if incomplete == 0:
        status_color = GREEN
        status_text = f"All {total} task(s) completed ✓"
    else:
        status_color = YELLOW
        status_text = f"{completed} of {total} task(s) completed ({incomplete} remaining)"

    lines.append(f"Tasks Completed: {status_color}{status_text}{RESET}")
    lines.append("")

    # Files modified
    if modified_files:
        lines.append("Files Modified:")
        files_to_show = modified_files[:10]  # Limit to first 10 files
        for file_path in files_to_show:
            lines.append(f"  {BLUE}- {file_path}{RESET}")

        if len(modified_files) > 10:
            remaining = len(modified_files) - 10
            lines.append(f"  ... and {remaining} more file(s)")
        lines.append("")
    else:
        lines.append(f"{YELLOW}No files modified during implementation{RESET}")
        lines.append("")

    # Overall status
    if incomplete == 0:
        overall_status = f"{GREEN}Status: Ready for review{RESET}"
    else:
        overall_status = f"{YELLOW}Status: Some tasks incomplete - review may be needed{RESET}"

    lines.append(overall_status)
    lines.append("")
    lines.append(format_separator("=", 70))

    return "\n".join(lines)


def run_review(verbose: bool = False, num_reviewers: int = 0) -> tuple[int, str]:
    """
    Run acr --local to perform code review.

    Args:
        verbose: Enable verbose logging
        num_reviewers: Number of reviewers to use (0 = use acr default)

    Returns:
        tuple of (exit_code, output)
        - exit_code 0: no findings (LGTM)
        - exit_code 1: findings found
        - exit_code 2: error
    """
    log("REVIEW", "Running code review with acr...")

    cmd = ["acr", "--local"]
    if num_reviewers > 0:
        cmd.extend(["--reviewers", str(num_reviewers)])

    if verbose:
        log("REVIEW", f"Running: {' '.join(cmd)}")

    result = run_command(cmd)
    output = result.stdout + result.stderr

    if result.returncode == 0:
        log("REVIEW", "LGTM - No issues found!")
    elif result.returncode == 1:
        log("REVIEW", "Issues found in review")
    else:
        log("REVIEW", f"Review error (exit code {result.returncode})")

    return result.returncode, output


def triage_and_fix_issues(review_output: str, verbose: bool = False, logger: Optional[Any] = None, model: str = "claude") -> None:
    """
    Triage review findings and fix real issues or add comments for false positives.
    """
    log("TRIAGE", "Analyzing and addressing review findings...")

    if logger:
        logger.log_event("triage_start", {"review_output_length": len(review_output)})

    triage_prompt = f"""# Code Review Triage and Fix

You have received the following code review findings from an automated review tool.
Your task is to analyze each finding and take appropriate action:

## Review Findings
```
{review_output}
```

## Instructions

For each finding above:

1. **Analyze** the finding carefully to determine if it's:
   - A **real issue** that needs to be fixed
   - A **false positive** where the current code is actually correct

2. **For real issues**: Fix the code to address the concern. Make the minimal change needed.

3. **For false positives**: Add an explanatory comment at the location of the finding.
   Use the appropriate comment syntax for the file type (e.g., `// NOTE:` for JS/TS/Java/C,
   `# NOTE:` for Python/Shell, `/* NOTE: */` for CSS/JSON when supported).
   The comment should be clear enough that both humans and automated tools understand
   why this code is correct as-is.

## Important

- Make a judgment call on each finding - not all review findings are valid
- Be concise in your explanations
- Only modify files that are directly related to the findings
- Log which findings you classified as real issues vs false positives

Begin analyzing and fixing the findings now.
"""

    cmd = [model, "--dangerously-skip-permissions", "-p", triage_prompt]

    if verbose:
        log("TRIAGE", f"Running: {' '.join(cmd)}")

    if logger:
        logger.log_command(cmd, start=True)

    import time
    start_time = time.time()

    result = run_command(cmd)

    duration = time.time() - start_time

    if logger:
        logger.log_command(
            cmd,
            output=result.stdout + result.stderr,
            exit_code=result.returncode,
            duration=duration
        )

    if result.returncode != 0:
        log("TRIAGE", f"Error during triage: {result.stderr}")
        if logger:
            logger.log_error("triage_failed", error=result.stderr, exit_code=result.returncode)
        # Don't exit - continue with the loop and try review again
    else:
        log("TRIAGE", "Triage and fixes complete")
        if logger:
            logger.log_event("triage_complete", {"duration": duration})


def main() -> None:
    # Pre-parse to check for --verbose flag (needed for config loading)
    pre_parser = argparse.ArgumentParser(add_help=False)
    pre_parser.add_argument("--verbose", "-v", action="store_true")
    pre_args, _ = pre_parser.parse_known_args()

    # Load configuration from files
    feature_loop_config, acr_config, ralph_loop_config = load_configuration(verbose=pre_args.verbose)

    # Get defaults from config (fallback to hardcoded defaults)
    default_model = feature_loop_config.get("model", "claude")
    default_max_iterations = feature_loop_config.get("max_review_iterations", 5)
    default_max_implement_iterations = feature_loop_config.get("max_implement_iterations", 0)  # 0 = unlimited
    default_skip_review = feature_loop_config.get("skip_review", False)
    default_verbose = feature_loop_config.get("verbose", False) or pre_args.verbose
    default_num_reviewers = acr_config.get("num_reviewers", 0)  # 0 = use acr default

    if default_verbose and (feature_loop_config or acr_config):
        log("CONFIG", f"Using configuration: model={default_model}, "
                     f"max_review_iterations={default_max_iterations}, "
                     f"max_implement_iterations={default_max_implement_iterations}, "
                     f"skip_review={default_skip_review}, verbose={default_verbose}, "
                     f"acr.num_reviewers={default_num_reviewers}")

    parser = argparse.ArgumentParser(
        description="Automated SDLC feature planning, implementation, and review loop",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    %(prog)s "Add user authentication"
    %(prog)s --prompt "Add search" --max-review-iterations 3
    %(prog)s "Fix login bug" --skip-review
    %(prog)s "Add feature" --no-verbose  # Override config verbose=true
    %(prog)s "Add feature" --no-skip-review  # Override config skip_review=true
    %(prog)s "Add API endpoint" --model claude --log /tmp/feature.log

Configuration:
    Settings can be configured in .claude-kit files (JSON format).
    Precedence: CLI args > ./. claude-kit > git-root/.claude-kit > ~/.claude-kit > defaults

    Use --no-verbose and --no-skip-review to explicitly disable config defaults.

    See .claude-kit.example for configuration options.
        """
    )

    parser.add_argument(
        "prompt",
        nargs="?",
        help="The feature description to implement (optional when resuming)"
    )
    parser.add_argument(
        "--prompt", "-p",
        dest="prompt_flag",
        help="The feature description (alternative to positional arg)"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume from auto-detected checkpoint state"
    )
    parser.add_argument(
        "--resume-from",
        type=str,
        metavar="PATH",
        help="Resume from specific checkpoint state file"
    )
    parser.add_argument(
        "--no-resume",
        action="store_true",
        help="Force fresh start, ignore existing checkpoint"
    )
    parser.add_argument(
        "--max-review-iterations", "-m",
        type=int,
        default=default_max_iterations,
        help=f"Maximum review/fix cycles (default: {default_max_iterations})"
    )
    parser.add_argument(
        "--max-implement-iterations",
        type=int,
        default=default_max_implement_iterations,
        help=f"Maximum ralph loop iterations for implementation (0=unlimited, default: {default_max_implement_iterations})"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        dest="verbose",
        help="Enable verbose logging"
    )
    parser.add_argument(
        "--no-verbose",
        action="store_false",
        dest="verbose",
        help="Disable verbose logging (override config)"
    )
    parser.add_argument(
        "--skip-review",
        action="store_true",
        dest="skip_review",
        help="Skip the review loop (just plan and implement)"
    )
    parser.add_argument(
        "--no-skip-review",
        action="store_false",
        dest="skip_review",
        help="Enable review loop (override config)"
    )
    parser.add_argument(
        "--num-reviewers",
        type=int,
        default=default_num_reviewers,
        help=f"Number of acr reviewers (0=use acr default, default: {default_num_reviewers})"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=default_model,
        help=f"Model to use for agent invocations (default: {default_model})"
    )
    parser.add_argument(
        "--log", "--log-file",
        dest="log_file",
        type=str,
        default=None,
        help="Path to log file for detailed execution capture (optional)"
    )
    parser.set_defaults(
        verbose=default_verbose,
        skip_review=default_skip_review
    )

    args = parser.parse_args()

    # Validate conflicting resume flags
    if args.resume and args.no_resume:
        parser.error("Cannot specify both --resume and --no-resume")

    if args.resume_from and args.no_resume:
        parser.error("Cannot specify both --resume-from and --no-resume")

    if args.resume and args.resume_from:
        parser.error("Cannot specify both --resume and --resume-from")

    # Validate max_review_iterations is positive
    if args.max_review_iterations < 1:
        parser.error(f"--max-review-iterations must be a positive integer (>=1), got: {args.max_review_iterations}")

    # Validate num_reviewers is non-negative
    if args.num_reviewers < 0:
        parser.error(f"--num-reviewers must be a non-negative integer (>=0), got: {args.num_reviewers}")

    # Get the feature prompt from either positional or flag argument
    feature_prompt = args.prompt or args.prompt_flag

    # Prompt is required unless resuming
    if not feature_prompt and not args.resume and not args.resume_from:
        parser.error("Feature prompt is required (either positional or --prompt) unless resuming")

    # Check required tools exist
    # NOTE: We check for "claude" command here because it's the default. If a different model is
    # specified via args.model, it will be validated when actually executed. This is the most common
    # case and provides early feedback to users.
    if not check_command_exists("claude"):
        log("ERROR", "Claude CLI not found in PATH. Please install it first.")
        sys.exit(1)

    if not args.skip_review and not check_command_exists("acr"):
        log("ERROR", "acr CLI not found in PATH. Please install it or use --skip-review.")
        sys.exit(1)

    # Handle resume logic
    checkpoint_state: Optional[CheckpointState] = None
    resuming = False

    if args.resume or args.resume_from:
        # User explicitly wants to resume
        if args.resume_from:
            state_file = Path(args.resume_from)
        else:
            state_file = get_state_file_path(feature_prompt or "")

        checkpoint_state = load_checkpoint(state_file, args.verbose)

        if checkpoint_state is None:
            log("ERROR", f"Cannot resume: checkpoint file not found or invalid: {state_file}")
            sys.exit(1)

        # Validate resumed state
        plan_file = Path(checkpoint_state["plan_path"])
        if not plan_file.exists():
            log("ERROR", f"Cannot resume: plan file not found: {checkpoint_state['plan_path']}")
            sys.exit(1)

        # Use feature_prompt from checkpoint if not provided
        if not feature_prompt:
            feature_prompt = checkpoint_state["feature_prompt"]

        resuming = True
        log("RESUME", f"Resuming from checkpoint (phase: {checkpoint_state['phase']}, iteration: {checkpoint_state['review_iteration']})")

    elif not args.no_resume:
        # Check for existing state file and suggest resume
        # Only check if feature_prompt is set (can't resume without knowing which feature)
        if feature_prompt:
            state_file = get_state_file_path(feature_prompt)
            if state_file.exists():
                log("INFO", f"Found existing checkpoint: {state_file}")
                log("INFO", "Use --resume to continue from checkpoint, or --no-resume to start fresh")

    # Initialize logger if log file is provided
    logger = None
    if args.log_file:
        if StructuredLogger is None:
            log("WARNING", "Logger module not available - logging will be disabled")
        else:
            try:
                logger = StructuredLogger(args.log_file)
                logger.log_event("initialization", {
                    "message": "Feature loop started",
                    "feature_prompt": feature_prompt,
                })
            except Exception as e:
                log("WARNING", f"Failed to initialize logger: {e} - continuing without logging")
                logger = None

    # Print header
    print()
    print("=" * 50)
    print("  Feature Loop - SDLC Automation")
    print("=" * 50)
    print()
    log("CONFIG", f"Feature: {feature_prompt}")
    log("CONFIG", f"Max implement iterations: {args.max_implement_iterations} (0=unlimited)")
    log("CONFIG", f"Max review iterations: {args.max_review_iterations}")
    log("CONFIG", f"Skip review: {args.skip_review}")
    log("CONFIG", f"Num reviewers: {args.num_reviewers} (0=acr default)")
    log("CONFIG", f"Verbose: {args.verbose}")
    log("CONFIG", f"Model: {args.model}")
    if args.log_file:
        log("CONFIG", f"Log file: {args.log_file}")
    print()

    # Log configuration snapshot
    if logger:
        logger.log_configuration({
            "feature_prompt": feature_prompt,
            "model": args.model,
            "max_implement_iterations": args.max_implement_iterations,
            "max_review_iterations": args.max_review_iterations,
            "skip_review": args.skip_review,
            "num_reviewers": args.num_reviewers,
            "verbose": args.verbose,
            "log_file": args.log_file,
            "resuming": resuming,
            "checkpoint_phase": checkpoint_state["phase"] if checkpoint_state else None,
        })

    # Step 1: Create the plan
    # NOTE: The logic here checks if we should skip planning. If phase != "planning", we're past it.
    # This is correct - we don't want to re-run planning if we're already in implementing or reviewing.
    if resuming and checkpoint_state["phase"] != "planning":
        # Skip planning phase when resuming past it
        log("RESUME", "Skipping planning phase (already completed)")
        plan_path = checkpoint_state["plan_path"]
        print()
    else:
        print("-" * 50)
        print("  Phase 1: Planning")
        print("-" * 50)
        plan_path = run_feature_plan(feature_prompt, args.verbose, logger=logger, model=args.model)

        # Save checkpoint after planning
        # NOTE: checkpoint_state may be None here (fresh start) or a loaded checkpoint (resume).
        # The conditional checkpoint_state["started_at"] if checkpoint_state else datetime.now().isoformat()
        # handles both cases correctly without needing a None check before accessing the dict.
        current_config = {
            "max_review_iterations": args.max_review_iterations,
            "max_implement_iterations": args.max_implement_iterations,
            "skip_review": args.skip_review,
            "verbose": args.verbose,
            "num_reviewers": args.num_reviewers,
        }
        checkpoint_state = CheckpointState(
            version=CHECKPOINT_VERSION,
            feature_prompt=feature_prompt,
            plan_path=plan_path,
            phase="implementing",
            review_iteration=0,
            started_at=checkpoint_state["started_at"] if checkpoint_state else datetime.now().isoformat(),
            updated_at=datetime.now().isoformat(),
            config=current_config,
        )
        save_checkpoint(checkpoint_state, args.verbose)
        print()

    # Step 2: Implement the plan using ralph loop
    if resuming and checkpoint_state["phase"] == "reviewing":
        # Skip implementation phase when resuming in review phase
        log("RESUME", "Skipping implementation phase (already completed)")
        print()
    else:
        print("-" * 50)
        print("  Phase 2: Implementation (Ralph Loop)")
        print("-" * 50)

        # Capture git state before implementation
        import subprocess
        before_commit = None
        try:
            result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                capture_output=True,
                text=True,
                check=True,
            )
            before_commit = result.stdout.strip()
        except subprocess.CalledProcessError:
            # Not a git repo or git error - continue without tracking
            pass

        run_implement(plan_path, args.verbose, args.max_implement_iterations, logger=logger, model=args.model, log_file=args.log_file)

        # Generate and display implementation summary
        parsed_impl_data = parse_implementation_results(plan_path)
        modified_files = []
        if before_commit:
            modified_files = get_implementation_file_changes(before_commit)

        impl_summary = format_implementation_summary(parsed_impl_data, modified_files)
        print(impl_summary)

        # Log implementation summary to file
        log_path = get_feature_loop_log_path(feature_prompt)
        log_summary_to_file(log_path, impl_summary)

        # Save checkpoint after implementation
        checkpoint_state["phase"] = "reviewing"
        checkpoint_state["review_iteration"] = 0
        save_checkpoint(checkpoint_state, args.verbose)
        print()

    # Step 3: Review loop (unless skipped)
    if args.skip_review:
        log("SKIP", "Skipping review phase as requested")
    else:
        print("-" * 50)
        print("  Phase 3: Review Loop")
        print("-" * 50)

        # Determine starting iteration for review loop
        start_iteration = 1
        if resuming and checkpoint_state["phase"] == "reviewing":
            start_iteration = checkpoint_state["review_iteration"] + 1
            if start_iteration > 1:
                log("RESUME", f"Resuming review from iteration {start_iteration}")

            # Check if we've already exhausted max iterations
            if start_iteration > args.max_review_iterations:
                log("ERROR", f"Already reached max review iterations ({args.max_review_iterations})")
                log("ERROR", "Manual review recommended")
                sys.exit(1)

        for iteration in range(start_iteration, args.max_review_iterations + 1):
            log("LOOP", f"Review iteration {iteration}/{args.max_review_iterations}")

            exit_code, review_output = run_review(args.verbose, args.num_reviewers)

            # Parse and display ACR summary
            parsed_acr_data = parse_acr_output(review_output, exit_code)
            acr_summary = format_acr_summary(parsed_acr_data, iteration)
            print()
            print(acr_summary)
            print()

            # Log ACR summary to file
            log_path = get_feature_loop_log_path(feature_prompt)
            log_summary_to_file(log_path, acr_summary)

            if exit_code == 0:
                # LGTM - no issues found
                log("LOOP", "Code review passed!")
                break
            elif exit_code == 1:
                # Issues found - triage and fix
                log("LOOP", "Triaging and fixing issues...")
                triage_and_fix_issues(review_output, args.verbose, logger=logger, model=args.model)

                # Save checkpoint after review iteration
                checkpoint_state["review_iteration"] = iteration
                save_checkpoint(checkpoint_state, args.verbose)
            else:
                # Error during review
                log("ERROR", f"Review failed with exit code {exit_code}")
                log("ERROR", f"Output: {review_output}")
                sys.exit(exit_code)

            if iteration == args.max_review_iterations:
                log("LOOP", f"Max review iterations ({args.max_review_iterations}) reached")
                log("LOOP", "Some issues may remain - manual review recommended")
                print()
                print("=" * 50)
                print("  Feature Loop Complete (with warnings)")
                print("=" * 50)
                print()
                log("DONE", f"Plan file: {plan_path}")
                log("ERROR", "Max review iterations reached without LGTM")
                sys.exit(1)

        print()

    # Final status
    print("=" * 50)
    print("  Feature Loop Complete")
    print("=" * 50)
    print()
    log("DONE", f"Plan file: {plan_path}")
    log("DONE", "Feature loop finished successfully!")

    # Close logger if it was initialized
    if logger:
        logger.log_event("feature_loop_complete", {"status": "success"})
        logger.close()

    # Clear checkpoint on successful completion
    state_file = get_state_file_path(feature_prompt)
    clear_checkpoint(state_file, args.verbose)


if __name__ == "__main__":
    main()
