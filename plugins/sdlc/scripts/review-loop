#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.10"
# dependencies = []
# ///
"""
review-loop - Automated code review and triage loop.

This script runs an iterative code review loop:
1. Run ACR (Automated Code Review) with specified configuration
2. Parse findings from ACR output
3. Display structured review summary
4. Invoke Claude to diagnose findings (real issues vs false positives)
5. Fix real issues and add explanatory comments for false positives
6. Repeat until LGTM or max iterations reached

Usage:
    review-loop
    review-loop --max-iterations 3
    review-loop --num-reviewers 5 --model claude --verbose
    review-loop --log review-output.log
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Optional, TypedDict


class ReviewLoopConfig(TypedDict, total=False):
    """Configuration structure for review_loop section in .claude-kit."""
    model: str  # Model to use for triage (default: "claude")
    max_iterations: int  # Max review iterations
    verbose: bool  # Enable verbose logging


class ACRConfig(TypedDict, total=False):
    """Configuration structure for acr section in .claude-kit."""
    num_reviewers: int  # Number of reviewers for ACR (default: 5)


def log(step: str, message: str) -> None:
    """Log a message with timestamp and step indicator."""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] [{step}] {message}", flush=True)


def run_command(cmd: list[str], capture_output: bool = True, interactive: bool = False) -> subprocess.CompletedProcess:
    """Run a command and return the result.

    Args:
        cmd: Command and arguments to run
        capture_output: If True, capture stdout/stderr. If False, show output in terminal.
        interactive: If True, allow stdin from terminal. If False, detach stdin to prevent prompts.
    """
    stdin = None if interactive else subprocess.DEVNULL
    if capture_output:
        return subprocess.run(cmd, capture_output=True, text=True, stdin=stdin)
    else:
        return subprocess.run(cmd, text=True, stdin=stdin)


def check_command_exists(cmd: str) -> bool:
    """Check if a command exists in PATH."""
    result = subprocess.run(["which", cmd], capture_output=True, text=True)
    return result.returncode == 0


def find_git_root() -> Optional[Path]:
    """Find the git root directory, if in a git repository."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--show-toplevel"],
            capture_output=True,
            text=True,
            check=False
        )
        if result.returncode == 0:
            return Path(result.stdout.strip())
    except Exception:
        pass
    return None


def discover_config_files(verbose: bool = False) -> list[Path]:
    """
    Discover .claude-kit configuration files in order of precedence (lowest to highest).

    Search order:
    1. User home directory (~/.claude-kit)
    2. Git root directory (./.claude-kit)
    3. Current working directory (./.claude-kit)

    Returns list of existing config file paths in precedence order (lowest first).
    """
    config_files = []

    # 1. User home directory (lowest precedence)
    home_config = Path.home() / ".claude-kit"
    if home_config.exists():
        config_files.append(home_config)
        if verbose:
            log("CONFIG", f"Found config: {home_config}")

    # 2. Git root directory (if in a git repo and different from cwd)
    git_root = find_git_root()
    if git_root:
        git_config = git_root / ".claude-kit"
        cwd = Path.cwd()
        if git_config.exists() and git_root != cwd:
            config_files.append(git_config)
            if verbose:
                log("CONFIG", f"Found config: {git_config}")

    # 3. Current working directory (highest precedence)
    cwd_config = Path.cwd() / ".claude-kit"
    if cwd_config.exists():
        config_files.append(cwd_config)
        if verbose:
            log("CONFIG", f"Found config: {cwd_config}")

    return config_files


def load_config_file(config_path: Path, verbose: bool = False) -> dict[str, Any]:
    """
    Load and parse a configuration file.

    Returns the parsed configuration or an empty dict on error.
    """
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)

        if verbose:
            log("CONFIG", f"Loaded config from: {config_path}")

        return config
    except json.JSONDecodeError as e:
        log("ERROR", f"Invalid JSON in config file {config_path}: {e}")
        sys.exit(1)
    except Exception as e:
        log("ERROR", f"Error reading config file {config_path}: {e}")
        sys.exit(1)


def merge_configs(configs: list[dict[str, Any]]) -> dict[str, Any]:
    """
    Merge multiple configuration dictionaries with later configs taking precedence.

    Performs a deep merge of nested dictionaries.
    """
    result = {}

    for config in configs:
        for key, value in config.items():
            if isinstance(value, dict) and key in result and isinstance(result[key], dict):
                # Deep merge for nested dicts
                result[key] = {**result[key], **value}
            else:
                result[key] = value

    return result


def validate_config(config: dict[str, Any]) -> tuple[ReviewLoopConfig, ACRConfig]:
    """
    Validate configuration values and return validated settings.

    Returns a tuple of (review_loop_config, acr_config) or exits on validation error.
    """
    review_loop_config = config.get("review_loop", {})
    acr_config = config.get("acr", {})

    # Validate that review_loop is a dict/object, not array or other type
    if not isinstance(review_loop_config, dict):
        log("ERROR", f"review_loop config must be a JSON object (dict), got: {type(review_loop_config).__name__}")
        sys.exit(1)

    # Validate that acr is a dict/object, not array or other type
    if not isinstance(acr_config, dict):
        log("ERROR", f"acr config must be a JSON object (dict), got: {type(acr_config).__name__}")
        sys.exit(1)

    # Validate max_iterations
    if "max_iterations" in review_loop_config:
        value = review_loop_config["max_iterations"]
        if not isinstance(value, int) or value < 1:
            log("ERROR", f"max_iterations must be a positive integer, got: {value}")
            sys.exit(1)

    # Validate verbose
    if "verbose" in review_loop_config:
        value = review_loop_config["verbose"]
        if not isinstance(value, bool):
            log("ERROR", f"verbose must be a boolean (true/false), got: {value}")
            sys.exit(1)

    # Validate model
    if "model" in review_loop_config:
        value = review_loop_config["model"]
        if not isinstance(value, str) or not value.strip():
            log("ERROR", f"review_loop.model must be a non-empty string, got: {value}")
            sys.exit(1)

    # Validate acr.num_reviewers
    if "num_reviewers" in acr_config:
        value = acr_config["num_reviewers"]
        if not isinstance(value, int) or value < 1:
            log("ERROR", f"acr.num_reviewers must be a positive integer, got: {value}")
            sys.exit(1)

    return review_loop_config, acr_config


def load_configuration(verbose: bool = False) -> tuple[ReviewLoopConfig, ACRConfig]:
    """
    Load and merge configuration from all discovered config files.

    Returns a tuple of (review_loop_config, acr_config).
    Precedence: defaults < user home < git root < current dir < CLI args
    """
    config_files = discover_config_files(verbose)

    if not config_files and verbose:
        log("CONFIG", "No configuration files found, using defaults")

    # Load all config files
    configs = [load_config_file(path, verbose) for path in config_files]

    # Merge configurations
    merged_config = merge_configs(configs)

    # Validate and extract settings
    review_loop_config, acr_config = validate_config(merged_config)

    return review_loop_config, acr_config


def supports_color() -> bool:
    """
    Detect if the terminal supports ANSI color codes.

    Returns False if:
    - stdout is not a TTY (output is piped/redirected)
    - TERM environment variable is "dumb"
    - NO_COLOR environment variable is set

    Returns True otherwise.
    """
    # Check if NO_COLOR is set
    if os.environ.get("NO_COLOR"):
        return False

    # Check if stdout is a TTY
    if not sys.stdout.isatty():
        return False

    # Check TERM environment variable
    term = os.environ.get("TERM", "")
    if term == "dumb":
        return False

    return True


# Color constants - evaluated once at module load time
_COLOR_SUPPORT = supports_color()
GREEN = "\033[32m" if _COLOR_SUPPORT else ""
YELLOW = "\033[33m" if _COLOR_SUPPORT else ""
RED = "\033[31m" if _COLOR_SUPPORT else ""
BLUE = "\033[34m" if _COLOR_SUPPORT else ""
RESET = "\033[0m" if _COLOR_SUPPORT else ""


def format_separator(char: str = "=", width: int = 70) -> str:
    """
    Create a visual separator line.

    Args:
        char: Character to use for the separator (default: "=")
        width: Width of the separator line (default: 70)

    Returns:
        A string of repeated characters of the specified width
    """
    return char * width


def format_timestamp() -> str:
    """
    Format current timestamp for display in summaries.

    Returns:
        Formatted timestamp string (e.g., "2024-01-15 14:30:22")
    """
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def parse_acr_output(review_output: str, exit_code: int) -> dict[str, Any]:
    """
    Parse ACR review output to extract structured findings data.

    Args:
        review_output: Raw output from acr command (stdout + stderr)
        exit_code: Exit code from acr command (0=LGTM, 1=findings, 2=error)

    Returns:
        Dictionary with parsed data:
        - total_findings: int count of findings
        - files_with_issues: list of file paths that have issues
        - findings_per_file: dict mapping file path to count
        - finding_types: list of finding categories/types (if detectable)
        - is_lgtm: bool, True if exit code 0
        - raw_output: original output for fallback
    """
    parsed_data = {
        "total_findings": 0,
        "files_with_issues": [],
        "findings_per_file": {},
        "finding_types": [],
        "is_lgtm": exit_code == 0,
        "raw_output": review_output
    }

    # If LGTM, return early
    if exit_code == 0:
        return parsed_data

    # Parse file paths from output
    # Common patterns:
    # - "path/to/file.py:123: finding description"
    # - "path/to/file.py line 123: finding"
    # - "File: path/to/file.py"
    file_path_patterns = [
        r'^([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+):\d+:',  # file.py:123:
        r'^([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+)\s+line\s+\d+:',  # file.py line 123:
        r'File:\s*([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+)',  # File: file.py
        r'^\s*([a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+)\s*$',  # file.py on its own line
    ]

    files_found = set()
    file_finding_counts = {}

    for line in review_output.split('\n'):
        for pattern in file_path_patterns:
            match = re.search(pattern, line, re.MULTILINE)
            if match:
                file_path = match.group(1)
                files_found.add(file_path)
                file_finding_counts[file_path] = file_finding_counts.get(file_path, 0) + 1
                break

    # Try to extract finding types/categories
    # Look for common patterns like "Error:", "Warning:", "Security:", etc.
    finding_type_patterns = [
        r'(Error|Warning|Info|Security|Performance|Style|Bug|Critical):\s',
        r'\[(Error|Warning|Info|Security|Performance|Style|Bug|Critical)\]',
    ]

    finding_types_found = set()
    for line in review_output.split('\n'):
        for pattern in finding_type_patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                finding_types_found.add(match.group(1).capitalize())

    # Estimate total findings
    # Strategy: count lines that look like findings
    # Heuristic: lines with file:line: pattern or starting with "- "
    finding_line_patterns = [
        r'^[a-zA-Z0-9_./\-]+\.[a-zA-Z0-9]+:\d+:',  # file.py:123:
        r'^\s*-\s+\w',  # - Finding description
        r'^\s*\d+\.\s+\w',  # 1. Finding description
    ]

    finding_count = 0
    for line in review_output.split('\n'):
        for pattern in finding_line_patterns:
            if re.match(pattern, line):
                finding_count += 1
                break

    # If no findings detected via patterns but exit code is 1, set to unknown
    if finding_count == 0 and exit_code == 1:
        # Check if output contains words like "finding", "issue", "problem"
        if re.search(r'\b(finding|issue|problem|error|warning)s?\b', review_output, re.IGNORECASE):
            finding_count = len(files_found) if files_found else 1

    parsed_data["total_findings"] = finding_count
    parsed_data["files_with_issues"] = sorted(list(files_found))
    parsed_data["findings_per_file"] = file_finding_counts
    parsed_data["finding_types"] = sorted(list(finding_types_found))

    return parsed_data


def format_acr_summary(parsed_data: dict[str, Any], iteration: int) -> str:
    """
    Format ACR review summary for display.

    Args:
        parsed_data: Dictionary returned by parse_acr_output()
        iteration: Current review iteration number

    Returns:
        Formatted summary string with colors and visual separators
    """
    lines = []
    timestamp = format_timestamp()

    # Header with separator
    lines.append(format_separator("=", 70))
    lines.append(f"Code Review Summary - Iteration {iteration} - {timestamp}")
    lines.append(format_separator("=", 70))
    lines.append("")

    # Total findings count with color coding
    total_findings = parsed_data["total_findings"]
    if parsed_data["is_lgtm"]:
        findings_line = f"{GREEN}Total Findings: 0 (LGTM âœ“){RESET}"
        status_line = f"{GREEN}Status: LGTM - Code is ready!{RESET}"
    else:
        if total_findings == 0:
            findings_color = YELLOW
            findings_text = "Unknown number of findings"
        elif total_findings <= 5:
            findings_color = YELLOW
            findings_text = f"{total_findings} finding(s)"
        else:
            findings_color = RED
            findings_text = f"{total_findings} finding(s)"

        findings_line = f"{findings_color}Total Findings: {findings_text}{RESET}"
        status_line = f"{YELLOW}Status: Issues found - proceeding to triage{RESET}"

    lines.append(findings_line)
    lines.append("")

    # Files with issues
    if parsed_data["files_with_issues"]:
        lines.append("Affected Files:")
        files_to_show = parsed_data["files_with_issues"][:10]  # Limit to first 10 files
        for file_path in files_to_show:
            count = parsed_data["findings_per_file"].get(file_path, 0)
            count_text = f" ({count} finding(s))" if count > 0 else ""
            lines.append(f"  {BLUE}- {file_path}{count_text}{RESET}")

        if len(parsed_data["files_with_issues"]) > 10:
            remaining = len(parsed_data["files_with_issues"]) - 10
            lines.append(f"  ... and {remaining} more file(s)")
        lines.append("")

    # Finding types/categories
    if parsed_data["finding_types"]:
        types_str = ", ".join(parsed_data["finding_types"])
        lines.append(f"Finding Categories: {types_str}")
        lines.append("")

    # Status
    lines.append(status_line)
    lines.append("")
    lines.append(format_separator("=", 70))

    return "\n".join(lines)


def run_review(verbose: bool = False, num_reviewers: int = 0) -> tuple[int, str]:
    """
    Run acr --local to perform code review.

    Args:
        verbose: Enable verbose logging
        num_reviewers: Number of reviewers to use (0 = use acr default)

    Returns:
        tuple of (exit_code, output)
        - exit_code 0: no findings (LGTM)
        - exit_code 1: findings found
        - exit_code 2: error
    """
    log("REVIEW", "Running code review with acr...")

    cmd = ["acr", "--local"]
    if num_reviewers > 0:
        cmd.extend(["--reviewers", str(num_reviewers)])

    if verbose:
        log("REVIEW", f"Running: {' '.join(cmd)}")

    result = run_command(cmd)
    output = result.stdout + result.stderr

    if result.returncode == 0:
        log("REVIEW", "LGTM - No issues found!")
    elif result.returncode == 1:
        log("REVIEW", "Issues found in review")
    else:
        log("REVIEW", f"Review error (exit code {result.returncode})")

    return result.returncode, output


def triage_and_fix_issues(review_output: str, verbose: bool = False, model: str = "claude") -> None:
    """
    Triage review findings and fix real issues or add comments for false positives.

    Args:
        review_output: Raw output from ACR containing findings
        verbose: Enable verbose logging
        model: Model to use for triage (default: "claude")
    """
    log("TRIAGE", "Analyzing and addressing review findings...")

    triage_prompt = f"""# Code Review Triage and Fix

You have received the following code review findings from an automated review tool.
Your task is to analyze each finding and take appropriate action:

## Review Findings
```
{review_output}
```

## Instructions

For each finding above:

1. **Analyze** the finding carefully to determine if it's:
   - A **real issue** that needs to be fixed
   - A **false positive** where the current code is actually correct

2. **For real issues**: Fix the code to address the concern. Make the minimal change needed.

3. **For false positives**: Add an explanatory comment at the location of the finding.
   Use the appropriate comment syntax for the file type (e.g., `// NOTE:` for JS/TS/Java/C,
   `# NOTE:` for Python/Shell, `/* NOTE: */` for CSS/JSON when supported).
   The comment should be clear enough that both humans and automated tools understand
   why this code is correct as-is.

## Important

- Make a judgment call on each finding - not all review findings are valid
- Be concise in your explanations
- Only modify files that are directly related to the findings
- Log which findings you classified as real issues vs false positives

Begin analyzing and fixing the findings now.
"""

    cmd = [model, "--dangerously-skip-permissions", "-p", triage_prompt]

    if verbose:
        log("TRIAGE", f"Running: {' '.join(cmd)}")

    import time
    start_time = time.time()

    result = run_command(cmd)

    duration = time.time() - start_time

    if result.returncode != 0:
        log("TRIAGE", f"Error during triage: {result.stderr}")
        # Don't exit - continue with the loop and try review again
    else:
        log("TRIAGE", f"Triage and fixes complete (took {duration:.1f}s)")

    if verbose:
        log("TRIAGE", f"Triage duration: {duration:.1f}s, exit code: {result.returncode}")


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Run automated code review loop with ACR",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s
  %(prog)s --max-iterations 3
  %(prog)s --num-reviewers 5 --model claude
  %(prog)s --verbose --log review.log
        """
    )

    parser.add_argument(
        "--max-iterations", "-m",
        type=int,
        default=5,
        help="Maximum review/fix cycles (default: 5)"
    )

    parser.add_argument(
        "--num-reviewers", "-r",
        type=int,
        default=5,
        help="Number of ACR reviewers (default: 5)"
    )

    parser.add_argument(
        "--model",
        type=str,
        default="claude",
        help="Model to use for triage (default: claude)"
    )

    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Enable verbose logging"
    )

    parser.add_argument(
        "--log", "--log-file",
        type=str,
        dest="log_file",
        help="Path to log file for detailed execution capture"
    )

    return parser.parse_args()


def main() -> int:
    """Main entry point for review-loop."""
    args = parse_arguments()

    # Load configuration from .claude-kit files
    review_loop_config, acr_config = load_configuration(verbose=args.verbose)

    # Merge CLI arguments with config (CLI args take precedence)
    # max_iterations: CLI arg overrides config
    max_iterations = args.max_iterations
    if max_iterations == 5 and "max_iterations" in review_loop_config:
        # Default value from CLI, use config value
        max_iterations = review_loop_config["max_iterations"]

    # num_reviewers: CLI arg overrides config
    num_reviewers = args.num_reviewers
    if num_reviewers == 5 and "num_reviewers" in acr_config:
        # Default value from CLI, use config value
        num_reviewers = acr_config["num_reviewers"]

    # model: CLI arg overrides config
    model = args.model
    if model == "claude" and "model" in review_loop_config:
        # Default value from CLI, use config value
        model = review_loop_config["model"]

    # verbose: CLI flag overrides config
    verbose = args.verbose
    if not verbose and "verbose" in review_loop_config:
        # Flag not set, use config value
        verbose = review_loop_config["verbose"]

    # Display final configuration
    print(f"Review Loop Configuration:")
    print(f"  Max iterations: {max_iterations}")
    print(f"  Number of reviewers: {num_reviewers}")
    print(f"  Model: {model}")
    print(f"  Verbose: {verbose}")
    if args.log_file:
        print(f"  Log file: {args.log_file}")
    print()

    if verbose:
        log("CONFIG", "Configuration loading complete")

    # TODO: Implement review execution
    # TODO: Implement triage logic
    # TODO: Implement main review loop
    # TODO: Implement logging

    print("Review loop implementation in progress...")
    return 0


if __name__ == "__main__":
    sys.exit(main())
